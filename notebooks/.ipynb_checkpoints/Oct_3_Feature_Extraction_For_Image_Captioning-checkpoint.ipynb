{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FmerCZWEizN"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from github (Original dataset)\n",
    "# !pip install wget\n",
    "# import wget\n",
    "# import zipfile\n",
    "\n",
    "# wget.download('https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip')\n",
    "# wget.download('https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip')\n",
    "\n",
    "\n",
    "# zip_flicker_dataset = '/content/drive/MyDrive/Data/Flickr8k_Dataset.zip'\n",
    "# zip_flicker_text = '/content/drive/MyDrive/Data/Flickr8k_text.zip'\n",
    "\n",
    "# with zipfile.ZipFile(zip_flicker_dataset) as zip:\n",
    "#   zip.extractall('/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Data')\n",
    "# with zipfile.ZipFile(zip_flicker_text) as zip:\n",
    "#   zip.extractall('/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD_84Ne0SOu0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Flatten\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "VOCAB_SIZE = 8800\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_DATA_DIR = '/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Data/Flicker8k_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVCwQOhNrgLI",
    "outputId": "1a7b4f1f-1f79-4551-8419-7047c4be434a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 4s 0us/step\n",
      "553476096/553467096 [==============================] - 4s 0us/step\n",
      "input_1\n",
      "1 block1_conv1 (3, 3, 3, 64)\n",
      "2 block1_conv2 (3, 3, 64, 64)\n",
      "block1_pool\n",
      "4 block2_conv1 (3, 3, 64, 128)\n",
      "5 block2_conv2 (3, 3, 128, 128)\n",
      "block2_pool\n",
      "7 block3_conv1 (3, 3, 128, 256)\n",
      "8 block3_conv2 (3, 3, 256, 256)\n",
      "9 block3_conv3 (3, 3, 256, 256)\n",
      "block3_pool\n",
      "11 block4_conv1 (3, 3, 256, 512)\n",
      "12 block4_conv2 (3, 3, 512, 512)\n",
      "13 block4_conv3 (3, 3, 512, 512)\n",
      "block4_pool\n",
      "15 block5_conv1 (3, 3, 512, 512)\n",
      "16 block5_conv2 (3, 3, 512, 512)\n",
      "17 block5_conv3 (3, 3, 512, 512)\n",
      "block5_pool\n",
      "flatten\n",
      "fc1\n",
      "fc2\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(include_top=True)\n",
    "\n",
    "for i in range(len(base_model.layers)):\n",
    "    curr_layer = base_model.layers[i]\n",
    "    if 'conv' not in curr_layer.name:\n",
    "        print(curr_layer.name)\n",
    "        # to get config print -> curr_layer.get_config()\n",
    "        continue\n",
    "    \n",
    "    filters, biasis = curr_layer.get_weights()\n",
    "    \n",
    "    print(i, curr_layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srZs-J_-FfeP"
   },
   "outputs": [],
   "source": [
    "# For extracting every layers output in vgg16\n",
    "features_list = [layer.output for layer in base_model.layers]\n",
    "feat_extraction_model = tf.keras.Model(inputs=base_model.input, outputs=features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEPjfg33F-OE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B2sBe8x-bng",
    "outputId": "e6f62650-964d-4dea-8112-9d5ed52cc380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 4096), dtype=tf.float32, name=None), name='fc2/Relu:0', description=\"created by layer 'fc2'\")\n"
     ]
    }
   ],
   "source": [
    "print(base_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Im9Cq9Pfvv-"
   },
   "source": [
    "## BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8frsq8GaVIBP",
    "outputId": "abc55c52-7ec5-43f9-b5ed-a3d3ad1928ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 8s 0us/step\n",
      "553476096/553467096 [==============================] - 8s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 0\n",
      "Non-trainable params: 138,357,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(include_top=True)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Taking till Flaten layer\n",
    "model = Model(inputs = base_model.inputs, outputs = base_model.layers[-2].output)\n",
    "\n",
    "img = image.img_to_array(image.load_img('/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Data/Flicker8k_Dataset/35506150_cbdb630f4f.jpg', target_size=(224, 224)))\n",
    "img = img.reshape(-1, 224, 224, 3)\n",
    "img = preprocess_input(img)\n",
    "\n",
    "# model.summary()\n",
    "img_features = model.predict(img)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ECwQ1u5f4xN",
    "outputId": "ddc5f4f7-2480-4f78-bec7-0b576e54f226"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# saving model\n",
    "# model.save('/content/drive/MyDrive/Data/Image-Captioning-Data/vgg16_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8T0tV9QwdmqI",
    "outputId": "e18ac740-8a8c-4b54-e284-41943c464445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (no. of samples, features)\n",
    "img_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx3Bi57d7cY0"
   },
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1n1VFM97lD8"
   },
   "outputs": [],
   "source": [
    "# features = {}\n",
    "\n",
    "# for full_img_name in tqdm(os.listdir(IMAGE_DATA_DIR)):\n",
    "#     # removing image format from image name\n",
    "#     img_name = full_img_name.split('.')[0]\n",
    "\n",
    "#     img_dir = os.path.join(IMAGE_DATA_DIR, full_img_name)\n",
    "\n",
    "#     # loading image and converting to numpy array and reshaping it to (224, 224, 3)\n",
    "#     # Image size is 224 because thats the image size in which vgg16 is trained on\n",
    "#     loaded_img = image.img_to_array(image.load_img(img_dir, target_size=(224, 224))).reshape(-1, 224, 224, 3)\n",
    "    \n",
    "#     #preprocessing data according to vgg16\n",
    "#     loaded_img = preprocess_input(loaded_img)\n",
    "\n",
    "#     features[img_name] = model.predict(loaded_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMVOzxXPcsQ-"
   },
   "source": [
    "## Saving features and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFTPo9pwMv05"
   },
   "outputs": [],
   "source": [
    "# Saving features\n",
    "# pickle.dump(features, open('/content/drive/MyDrive/Data/Image-Captioning-Data/features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBnZcRlwMvnZ"
   },
   "outputs": [],
   "source": [
    "# pickle.dump(model, open('/content/drive/MyDrive/Data/Image-Captioning-Data/model.h5', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WI9PXC4B5fC0"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZaMl01sMvkn"
   },
   "outputs": [],
   "source": [
    "mx_caption_length = 0\n",
    "def load_caption_data(filepath):\n",
    "    caption_mapping = {}\n",
    "    global mx_caption_length\n",
    "\n",
    "    # Reading caption file\n",
    "    with open(filepath, 'r') as file:\n",
    "        captions = file.read()\n",
    "\n",
    "    #looping through every line\n",
    "    for line in captions.split('\\n'):\n",
    "        #spliting b/w captions and image name\n",
    "        split_line = line.split()\n",
    "        if len(split_line) <= 3:\n",
    "            print(split_line)\n",
    "            continue\n",
    "        # fetching image name without format\n",
    "        img_name = split_line[0].split('.')[0]\n",
    "        # fetching caption\n",
    "        curr_caption = ' '.join(split_line[1:])\n",
    "        mx_caption_length = max(mx_caption_length, len(split_line))\n",
    "\n",
    "        # if img_name key is not in dict then declared that key\n",
    "        if img_name not in caption_mapping:\n",
    "            caption_mapping[img_name] = []\n",
    "        # appending every caption for an img_name\n",
    "        caption_mapping[img_name].append(curr_caption)\n",
    "    \n",
    "    return caption_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGf3qASkMvh2",
    "outputId": "5d5c4947-a977-46ba-bf9e-e2e524819c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2165461920_1a4144eb2b.jpg#0', 'dogs', 'racing']\n",
      "['2428275562_4bde2bc5ea.jpg#0', 'A']\n",
      "['244760301_5809214866.jpg#3', 'People', 'walking']\n",
      "['256085101_2c2617c5d0.jpg#3', 'Dog', 'yawns']\n",
      "['2714703706_d21c5cb8df.jpg#0', 'dogs', 'playing']\n",
      "['2755314937_1e974bf2b5.jpg#3', 'broken', 'image']\n",
      "['2862481071_86c65d46fa.jpg#4', 'Trucks', 'racing']\n",
      "['2929669711_b2d5a640f0.jpg#4', 'man', 'surfing']\n",
      "['3108732084_565b423162.jpg#2', 'a', 'snowboarder']\n",
      "['3125309108_1011486589.jpg#2', 'rugby', 'match']\n",
      "['3154693053_cfcd05c226.jpg#0', 'A', 'basketball']\n",
      "['3189251454_03b76c2e92.jpg#3', 'dog', 'barking']\n",
      "['3237760601_5334f3f3b5.jpg#1', 'A', 'skier']\n",
      "['3360823754_90967276ec.jpg#3', 'Man', 'skateboarding']\n",
      "['3640443200_b8066f37f6.jpg#0', 'a']\n",
      "['3664928753_7b0437fedf.jpg#3', 'Javelin', 'competition']\n",
      "['3694071771_ce760db4c7.jpg#0', 'a', 'cyclist']\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_caption_data('/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Text/Flickr8k.token.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyNByEu6MvfZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAZmm9G8Mvci"
   },
   "outputs": [],
   "source": [
    "base_model_out = base_model.output\n",
    "base_model_out = tf.keras.layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udAwSrv7NaW2"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(base_model.input, base_model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROc05TcxNhWW",
    "outputId": "ca464312-4bf2-459d-bca4-daf4035a007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 1000)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 0\n",
      "Non-trainable params: 138,357,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCZ5gT7qSHHx"
   },
   "outputs": [],
   "source": [
    "# np.save('test3.npy', input_vocab)    # .npy extension is added if not given\n",
    "# d = np.load('test3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMT1lvuafoh8"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHyXCN8KOxHM",
    "outputId": "bb32f2ac-9f2c-427d-f0b1-fa7ec69b705a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "decoder = load_model('/content/drive/MyDrive/Data/Image-Captioning-Data/model_30_sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPqshVYqPNyo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOoVM69-QhhI",
    "outputId": "38a60137-8c15-4dbb-edc7-712034d3d80a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3]], shape=(1, 1), dtype=int64)\n",
      "tf.Tensor([[1.8065745 0.        0.        ... 0.        0.        1.5827557]], shape=(1, 4096), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant([[3]], dtype='int64'))\n",
    "print(tf.constant(img_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuVPOw9jPGVm",
    "outputId": "da99d2f3-bf73-4d5d-ba95-5493a7cebdce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8800), dtype=float32, numpy=\n",
       "array([[2.1848898e-07, 1.0186731e-11, 8.6593431e-01, ..., 7.6171786e-12,\n",
       "        1.0512298e-11, 3.1581465e-12]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(tf.constant([[3]], dtype='int64'), tf.constant(img_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBJ_v9OzPXDh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
