{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EG0DMfBbLOiC",
    "outputId": "b047616c-40d8-469c-dcb4-708a444bf4f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuoW8FWSAvez"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import Checkpoint, CheckpointManager\n",
    "from keras import Input, Model\n",
    "from keras.layers import TextVectorization, Embedding, Flatten, Dense, Dropout, LSTM, GRU, add, StringLookup\n",
    "from keras.models import Sequential\n",
    "from keras.utils.all_utils import plot_model, to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tqdm import tqdm\n",
    "from tensorflow import GradientTape\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "FEATURES_PATH = '/content/drive/MyDrive/Data/Image-Captioning-Data/features.pkl'\n",
    "CAPTIONS_PATH = '/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Text/Flickr8k.token.txt'\n",
    "CHECKPOINT_PATH = '/content/drive/MyDrive/Data/Image-Captioning-Data/checkpoint/train'\n",
    "\n",
    "VOCAB_SIZE = 8800\n",
    "EMBED_SIZE = 256\n",
    "MAX_CAP_LEN = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Uq5w8oHCxDJ"
   },
   "source": [
    "## Encoder \n",
    "- Feature extracted from VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8Y4LqgSCDhb",
    "outputId": "f5156f1b-5c3d-4f30-c297-453a5ff3d84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 7s 0us/step\n",
      "553476096/553467096 [==============================] - 7s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 0\n",
      "Non-trainable params: 138,357,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(include_top=True)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Taking till Flaten layer\n",
    "model = Model(inputs = base_model.inputs, outputs = base_model.layers[-2].output)\n",
    "\n",
    "# model.summary()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlcvcD-SCJBY"
   },
   "outputs": [],
   "source": [
    "# img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aw31sEgsAyK8"
   },
   "outputs": [],
   "source": [
    "features = pickle.load(open(FEATURES_PATH, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4eEWg4SYT8t"
   },
   "source": [
    "## Preprocessing Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Oz9ad08ARf0"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary of 'captions' with 'image_id' as a key\n",
    "def load_caption_data(filepath):\n",
    "    caption_mapping = {}\n",
    "\n",
    "    # Reading caption file\n",
    "    with open(filepath, 'r') as file:\n",
    "        captions = file.read()\n",
    "\n",
    "    #looping through every line\n",
    "    for line in captions.split('\\n'):\n",
    "        #spliting b/w captions and image name\n",
    "        split_line = line.split()\n",
    "        if len(split_line) <= 3:\n",
    "            print(split_line)\n",
    "            continue\n",
    "        # fetching image name without format\n",
    "        img_name = split_line[0].split('.')[0]\n",
    "        # fetching caption\n",
    "        curr_caption = ' '.join(split_line[1:])\n",
    "\n",
    "        # if img_name key is not in dict then declared that key\n",
    "        if img_name not in caption_mapping:\n",
    "            caption_mapping[img_name] = []\n",
    "        # appending every caption for an img_name\n",
    "        caption_mapping[img_name].append(curr_caption)\n",
    "    \n",
    "    return caption_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_RRk3llBXQz",
    "outputId": "24a69aa3-6c18-4955-93a5-516e9677ca05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2165461920_1a4144eb2b.jpg#0', 'dogs', 'racing']\n",
      "['2428275562_4bde2bc5ea.jpg#0', 'A']\n",
      "['244760301_5809214866.jpg#3', 'People', 'walking']\n",
      "['256085101_2c2617c5d0.jpg#3', 'Dog', 'yawns']\n",
      "['2714703706_d21c5cb8df.jpg#0', 'dogs', 'playing']\n",
      "['2755314937_1e974bf2b5.jpg#3', 'broken', 'image']\n",
      "['2862481071_86c65d46fa.jpg#4', 'Trucks', 'racing']\n",
      "['2929669711_b2d5a640f0.jpg#4', 'man', 'surfing']\n",
      "['3108732084_565b423162.jpg#2', 'a', 'snowboarder']\n",
      "['3125309108_1011486589.jpg#2', 'rugby', 'match']\n",
      "['3154693053_cfcd05c226.jpg#0', 'A', 'basketball']\n",
      "['3189251454_03b76c2e92.jpg#3', 'dog', 'barking']\n",
      "['3237760601_5334f3f3b5.jpg#1', 'A', 'skier']\n",
      "['3360823754_90967276ec.jpg#3', 'Man', 'skateboarding']\n",
      "['3640443200_b8066f37f6.jpg#0', 'a']\n",
      "['3664928753_7b0437fedf.jpg#3', 'Javelin', 'competition']\n",
      "['3694071771_ce760db4c7.jpg#0', 'a', 'cyclist']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "caption_mapping = load_caption_data(CAPTIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMlOLqo_BiS8",
    "outputId": "112e56c9-2fc1-4af2-c520-2a3e58d99428"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8092"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of images\n",
    "len(caption_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN6NVZgiJR2v",
    "outputId": "d0827bf3-d377-4115-cd8d-84c73de40f1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking captions of a perticular 'image_id'\n",
    "caption_mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIP-KLFSCFCs"
   },
   "outputs": [],
   "source": [
    "#  Caption Preprocessing :-\n",
    "# - converting to lower case\n",
    "# - removing numbers, special charectors\n",
    "# - removing last and first whitespaces\n",
    "# - concatenating '<start>' and '<end>' strings\n",
    "\n",
    "def preprocessing_captions(caption_mapping):\n",
    "    # iterating over caption mapping\n",
    "    for img_name, captions in caption_mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            # changing upper-case char to lower-case\n",
    "            caption = caption.lower()\n",
    "\n",
    "            # using regex removing special characters and numbers\n",
    "            caption = re.sub('[^a-z ]', '', caption)\n",
    "            \n",
    "            # To remove front and back white spaces\n",
    "            caption = caption.strip()\n",
    "\n",
    "            #adding <start> and <end> string this will help in lstm\n",
    "            captions[i] = '<start> ' + caption + ' <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CuPiwoZHJAH"
   },
   "outputs": [],
   "source": [
    "preprocessing_captions(caption_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zvz2HQgHHMGP",
    "outputId": "842b00bb-2a7c-47bf-a8a7-1cadc231633b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> a child in a pink dress is climbing up a set of stairs in an entry way <end>',\n",
       " '<start> a girl going into a wooden building <end>',\n",
       " '<start> a little girl climbing into a wooden playhouse <end>',\n",
       " '<start> a little girl climbing the stairs to her playhouse <end>',\n",
       " '<start> a little girl in a pink dress going into a wooden cabin <end>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking changes in a particular image\n",
    "caption_mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zwd3hy01Ji1L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlzIHnFYXY1z"
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SWdF27fYEz7"
   },
   "outputs": [],
   "source": [
    "# copying image_id in a list and spliting image_id into train list and test list\n",
    "all_image_keys = [key for key in caption_mapping.keys()]\n",
    "# I have done 0.8 train and 0.2 test split\n",
    "till = int(len(caption_mapping) * 0.8)\n",
    "\n",
    "np.random.seed(18)\n",
    "np.random.shuffle(all_image_keys)\n",
    "\n",
    "train = all_image_keys[:till]\n",
    "test = all_image_keys[till:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qBM4dteZEwE",
    "outputId": "1b2847ca-74ff-4faf-f5ca-4b6ede0dfa7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  6473\n",
      "Test:  1619\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', len(train))\n",
    "print('Test: ', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVNKekD_bpew",
    "outputId": "c9f69218-764f-4915-8295-1f5f765cb72b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.507647, 0.      , 0.      , ..., 0.      , 0.      , 0.      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSYjWoJnuGx-",
    "outputId": "dd6f52af-d5cb-45bf-ba86-a609a4215fc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> a child in a pink dress is climbing up a set of stairs in an entry way <end>',\n",
       " '<start> a girl going into a wooden building <end>',\n",
       " '<start> a little girl climbing into a wooden playhouse <end>',\n",
       " '<start> a little girl climbing the stairs to her playhouse <end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing all captions in a dictionary\n",
    "all_captions = []\n",
    "\n",
    "for key, captions in caption_mapping.items():\n",
    "    for caption in captions:\n",
    "        all_captions.append(caption)\n",
    "all_captions[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFStBxitFY11",
    "outputId": "a3988be8-7179-440c-da05-2edb6c34e434"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40443"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNRF-CPXZtB7"
   },
   "outputs": [],
   "source": [
    "#           X                      y\n",
    "\n",
    "#   '<start>'                     'a'\n",
    "#   '<start>', 'a'                'child'\n",
    "#   '<start>', 'a', 'child'       'in' \n",
    "\n",
    "\n",
    "# Using 'TextVectorization' for representing words in integer\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length=40,\n",
    "    standardize=None\n",
    ")\n",
    "\n",
    "vectorization.adapt(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFAUWD_O2lKe",
    "outputId": "706c2fde-af19-4411-9089-bf0eb063daeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(40,), dtype=int64, numpy=\n",
       "array([1127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "vectorization('boxing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ak42JOVw2FV3",
    "outputId": "82c19594-41f3-4ad6-a238-777d17c52d25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<start> a child in a pink dress is climbing up a set of stairs in an entry way <end>'\n"
     ]
    }
   ],
   "source": [
    "cap_dataset = tf.data.Dataset.from_tensor_slices(all_captions)\n",
    "for x in cap_dataset.as_numpy_iterator():\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_JZCtBclRBV",
    "outputId": "cfe5e76e-21aa-406c-a3e5-96737859ef29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f5ed8348350>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEwjNfkLpM7X",
    "outputId": "3528c892-6287-4d14-98ba-a8cf3d622977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '[UNK]' 'a' '<start>' '<end>' 'in' 'the' 'on' 'is']\n",
      "Vocab Size:  8781\n",
      "Caption in integer:  tf.Tensor(\n",
      "[   3    2   43    5    2   91  171    8  120   54    2  397   13  395\n",
      "    5   29 7731  695    4    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int64)\n",
      "Caption in words:  <start> a child in a pink dress is climbing up a set of stairs in an entry way <end>                     \n"
     ]
    }
   ],
   "source": [
    "# storing all vocabulary in a numpy array\n",
    "input_vocab = np.array(vectorization.get_vocabulary())\n",
    "print(input_vocab[:9])\n",
    "print('Vocab Size: ', len(input_vocab))\n",
    "\n",
    "# converting integer to word\n",
    "print('Caption in integer: ', vectorization(caption_mapping['1000268201_693b08cb0e'][0]))\n",
    "tokens = input_vocab[vectorization(caption_mapping['1000268201_693b08cb0e'][0])]\n",
    "print('Caption in words: ', ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O168tQQJVBOA"
   },
   "outputs": [],
   "source": [
    "np.save('input_vocab.npy', input_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uchGyVU1FF4H"
   },
   "outputs": [],
   "source": [
    "#tensorflow.python.framework.ops.EagerTensor\n",
    "# tf.squeeze(vectorization(caption_mapping['1000268201_693b08cb0e'][0]))\n",
    "\n",
    "# checking how it will look like after converting into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0-IWOBrmSXZ"
   },
   "outputs": [],
   "source": [
    "# # created an object for converting index into corresponding word with the help of 'StringLookup' layer by keras\n",
    "# def idx_word_and_word_idx(vocabulary):\n",
    "#     index_to_word = StringLookup(vocabulary = vectorization.get_vocabulary(),\n",
    "#                                  oov_token='UNK',\n",
    "#                                  invert=True)\n",
    "#     word_to_index = StringLookup(vocabulary=vectorization.get_vocabulary())    \n",
    "#     return index_to_word, word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OncBtM3JmWwI"
   },
   "outputs": [],
   "source": [
    "# index_to_word, word_to_index = idx_word_and_word_idx(vectorization)\n",
    "\n",
    "# print(word_to_index('<start>'))\n",
    "# print(index_to_word(3))\n",
    "# print(vectorization('<start>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYQPrgtK1rRZ"
   },
   "source": [
    "## Dataset created with batch size of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnR6iZX1zwqY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def creating_dataset(all_keys):\n",
    "    img_vec, cap_vec = [], []\n",
    "    for img_key in tqdm(all_keys):\n",
    "        try:\n",
    "            # image feature is extracted \n",
    "            img_vec.extend([features[img_key][0]] * len(caption_mapping[img_key]))\n",
    "            # append label and captions\n",
    "            cap_vec.extend(vectorization(caption_mapping[img_key]))\n",
    "            # print(img_vec)\n",
    "            # print(cap_vec)\n",
    "        except:\n",
    "            print('\\nThis image have no Features: ', img_key)\n",
    "    return img_vec, cap_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciRTZByRA_iA",
    "outputId": "b4e41a81-ef53-490e-a981-19b82752b6c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 4454/6473 [00:27<00:11, 172.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This image have no Features:  2258277193_586949ec62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6473/6473 [00:38<00:00, 166.55it/s]\n",
      "100%|██████████| 1619/1619 [00:09<00:00, 170.80it/s]\n"
     ]
    }
   ],
   "source": [
    "img_train_feat, img_train_cap = creating_dataset(train)\n",
    "img_test_feat, img_test_cap = creating_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DisBYhOj8ukI",
    "outputId": "c619a877-31e5-4768-dc1b-d49e17cc3099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# length of feat and caption of one image\n",
    "print(len(img_train_feat[0]))\n",
    "print(len(img_train_cap[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cuHaIVtXbtL"
   },
   "source": [
    "## Previous Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRI2kProX_J4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqlmfyI1Xbqh"
   },
   "outputs": [],
   "source": [
    "# from keras.layers.merge import Add\n",
    "# # encoder\n",
    "# enc_input1 = Input(shape=(None, 4096))\n",
    "# enc_dropout1 = Dropout(0.4)(enc_input1)\n",
    "# enc_dense = Dense(512, activation='relu')(enc_dropout1)\n",
    "\n",
    "# # decoder\n",
    "# # input shape = (BATCH_SIZE, 40)\n",
    "# dec_input1 = Input(shape=(None, ))\n",
    "# dec_embed1 = Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True)(dec_input1)\n",
    "# dec_lstm1 = LSTM(512)(dec_embed1)\n",
    "# dec_dense1 = Dense(512)(dec_lstm1)\n",
    "\n",
    "# add_layer = add([dec_dense1, enc_dense])\n",
    "# dec_dense2 = Dense(VOCAB_SIZE, activation='softmax')(add_layer)\n",
    "\n",
    "# model = Model(inputs = [enc_input1, dec_input1], outputs=dec_dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENxAHcNSXblQ"
   },
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dY0R0srXbi5"
   },
   "outputs": [],
   "source": [
    "# def train_model(model):\n",
    "#     epochs = 1\n",
    "#     for epoch in range(epochs):\n",
    "#             x = creating_dataset(BATCH_SIZE, features, train, caption_mapping, vectorization, MAX_CAP_LEN)\n",
    "#             # print(x)\n",
    "#             # print(len(x[0]))\n",
    "#             # print(np.array(x[0][0]).shape)\n",
    "#             # print(np.array(x[0][1]).shape)\n",
    "#             # print(np.array(x[1]).shape)\n",
    "#             model.fit(x, epochs=1)\n",
    "#         # print(img_feat)\n",
    "#         # print(curr_caption)\n",
    "#         # print(curr_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0IWyK1Ci-TA"
   },
   "outputs": [],
   "source": [
    "# train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-wmaYSSXbgY"
   },
   "outputs": [],
   "source": [
    "# model.predict([np.array([temp_img_train[1]]), np.array([temp_cap_train[1]])]).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phUuRqcFX2TW"
   },
   "source": [
    "## Practicing LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUi6IpeMXbdv"
   },
   "outputs": [],
   "source": [
    "# x = [[x + 1, x + 2, x + 3] for x in range(1000)]\n",
    "# y = [[x] for x in range(4, 1004)]\n",
    "\n",
    "\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "# x = x.reshape([-1, 3, 1])\n",
    "# # print(x)\n",
    "# # print(y)\n",
    "\n",
    "# input = Input(shape=(3, 1))\n",
    "# lstm = LSTM(120)(input)\n",
    "# dropout = Dropout(0.4)(lstm)\n",
    "# dense = Dense(1)(dropout)\n",
    "# model = Model(inputs = input, outputs=dense)\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.losses.MeanSquaredError()])\n",
    "# model.fit(x, y, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS4kX7erX5v1"
   },
   "outputs": [],
   "source": [
    "# model.predict(np.array([[8, 9, 10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7ZpGcOPcHNT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_SUiitN5TjB"
   },
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbkN3mJv5Xim"
   },
   "outputs": [],
   "source": [
    "MAX_CAP_LEN = 40\n",
    "FEAT_SHAPE = 4096\n",
    "BATCH_SIZE = 64\n",
    "EMB_DIM = 256\n",
    "UNITS = 512\n",
    "NUM_STEPS = len(train)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFrYdxWj5X9g"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_train_feat, img_train_cap))\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(buffer_size=1000, seed=18)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSdmoc9aof_m"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, units, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.dropout0 = Dropout(0.5)\n",
    "        self.dense0 = Dense(self.units, activation='relu')\n",
    "\n",
    "        self.embed = Embedding(vocab_size, emb_dim)\n",
    "        self.gru = GRU(self.units)\n",
    "        self.dense1 = Dense(self.units, activation='relu')\n",
    "        self.dense2 = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, img_cap, feat):\n",
    "        # shape of img_cap (BatchSize, 1)\n",
    "        # shape of feat (BatchSize, 4096)\n",
    "\n",
    "        # 1. Image features from VGG16 \n",
    "        feat = self.dropout0(feat)\n",
    "        feat = self.dense0(feat)\n",
    "\n",
    "        # 2. Image Caption \n",
    "        img_cap = self.embed(img_cap)\n",
    "        img_cap = self.gru(img_cap)\n",
    "        \n",
    "        # adding output of image feature and next generated word for caption\n",
    "        img_cap = add([feat, img_cap])\n",
    "        \n",
    "        # passing it to dense layer\n",
    "        img_cap = self.dense1(img_cap)\n",
    "        img_cap = self.dense2(img_cap)\n",
    "\n",
    "        return img_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7A_yBzfokpN"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(UNITS, VOCAB_SIZE, EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5FLr-UUqq7-"
   },
   "outputs": [],
   "source": [
    "# checking with one image\n",
    "img_cap = decoder(np.array(img_train_cap[0][0]).reshape(1, -1), tf.expand_dims(img_train_feat[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SI-PN2FkwNX6",
    "outputId": "6c3831c8-7b08-43a2-f950-bfa783140cea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8800), dtype=float32, numpy=\n",
       "array([[1.6031823e-04, 2.1787336e-04, 1.0345922e-04, ..., 1.3741734e-04,\n",
       "        1.3352465e-04, 9.8713652e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "_8N8CB-Wre0c",
    "outputId": "8ab72d14-162d-4392-e99b-3b1bd3629f0b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'sphere'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab[np.argmax(img_cap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_RvOUMM9oCB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXCBYfAD9RPL"
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss = tf.keras.losses.categorical_crossentropy\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def cal_loss(target, pred):\n",
    "    curr_loss = loss(target, pred)\n",
    "    return curr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fB_m2QDME-_z"
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJKjDd9fDTD8"
   },
   "outputs": [],
   "source": [
    "# checkpoint is used to store all weights after every training\n",
    "ckpt = Checkpoint(optimizer = optimizer, model = decoder)\n",
    "ckpt_manager = CheckpointManager(ckpt, CHECKPOINT_PATH, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7ruHqN7FBmq"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8Iq4CPZ-iJP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "def train_per_batch(img_feats, img_caps):\n",
    "    batch_loss = 0\n",
    "\n",
    "    # GradientTape was used to update weights  \n",
    "    with GradientTape() as tape:\n",
    "        for i in range(MAX_CAP_LEN - 1):\n",
    "            # expanding dims of img_caps so shape will become (BatchSize, 1, 1)\n",
    "            curr_words = tf.expand_dims(img_caps[:, i], 1)\n",
    "            predictions = decoder(curr_words, img_feats)\n",
    "\n",
    "            # calculating current batch loss for one word and adding to var 'batch_loss'\n",
    "            true_label = to_categorical(img_caps[:, i+1], num_classes = VOCAB_SIZE)\n",
    "            batch_loss += tf.math.reduce_mean(cal_loss(true_label, predictions))\n",
    "    # To find trainable variables of this algorithm\n",
    "    # print(decoder.trainable_variables) \n",
    "\n",
    "    # fetching all trainable variable\n",
    "    trainable_variables = decoder.trainable_variables\n",
    "    # differentiating all trainable variables\n",
    "    gradient = tape.gradient(batch_loss, trainable_variables)\n",
    "    # applying 'Adam' optimizer \n",
    "    optimizer.apply_gradients(zip(gradient, trainable_variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoZMqImz5dGZ"
   },
   "outputs": [],
   "source": [
    "START_EPOCH = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    START_EPOCH = ckpt_manager.latest_checkpoint.split('-')[-1]\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpBj6p2c8T_A",
    "outputId": "51632f62-8f46-4d22-da5f-61ea1a08a859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 ,Batch: 0 ,Loss: 0.4167705774307251\n",
      "Epoch: 8 ,Batch: 100 ,Loss: 0.43797600269317627\n",
      "Epoch: 8 ,Batch: 200 ,Loss: 0.45435309410095215\n",
      "Epoch: 8 ,Batch: 300 ,Loss: 0.3950062096118927\n",
      "Epoch: 8 ,Batch: 400 ,Loss: 0.36970055103302\n",
      "Epoch: 8 ,Batch: 500 ,Loss: 0.4557041823863983\n",
      "Epoch: 9 ,Batch: 0 ,Loss: 0.41014397144317627\n",
      "Epoch: 9 ,Batch: 100 ,Loss: 0.4020518660545349\n",
      "Epoch: 9 ,Batch: 200 ,Loss: 0.3565552830696106\n",
      "Epoch: 9 ,Batch: 300 ,Loss: 0.37570860981941223\n",
      "Epoch: 9 ,Batch: 400 ,Loss: 0.3830665349960327\n",
      "Epoch: 9 ,Batch: 500 ,Loss: 0.37896305322647095\n",
      "Epoch: 10 ,Batch: 0 ,Loss: 0.3801550567150116\n",
      "Epoch: 10 ,Batch: 100 ,Loss: 0.4067823588848114\n",
      "Epoch: 10 ,Batch: 200 ,Loss: 0.35919395089149475\n",
      "Epoch: 10 ,Batch: 300 ,Loss: 0.34309062361717224\n",
      "Epoch: 10 ,Batch: 400 ,Loss: 0.3481622636318207\n",
      "Epoch: 10 ,Batch: 500 ,Loss: 0.3601483106613159\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for curr_epoch in range(int(START_EPOCH), epochs):\n",
    "    for (batch, (img_feats, img_caps)) in enumerate(dataset):\n",
    "        batch_loss = train_per_batch(img_feats, img_caps)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch:', curr_epoch + 1, ',Batch:', batch, ',Loss:', np.array(batch_loss)/int(img_caps.shape[0]))\n",
    "    ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "898JKre44tc3",
    "outputId": "ec5b9af4-0495-47ba-9121-c7bbe0341869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(decoder.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF-F8oJVufxj"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# train_var = decoder.trainable_variables\n",
    "# pickle.dump(train_var, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47UmD5fU436u",
    "outputId": "2c1af08f-1169-4214-d9d0-bef4359fb802"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3259231890_16fe167b31',\n",
       " '3671262694_29fbeb9d95',\n",
       " '477768471_d7cd618fdb',\n",
       " '2108799322_e25aa6e185']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHS1kJwW6Wav"
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFOQzFvwD_36"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_test_feat, img_test_cap))\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z72_Jp09wAXP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def open_img(img_name):\n",
    "    path = os.path.join('/content/drive/MyDrive/Data/Image-Captioning-Data/Flicker-Data/Flicker8k_Dataset', img_name + '.jpg')\n",
    "    img = Image.open(path)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCOPmS4EDzcO"
   },
   "outputs": [],
   "source": [
    "def evaluate(img_feat, img_cap):\n",
    "    curr_word = tf.expand_dims([vectorization('<start>')[0]], 1)\n",
    "    img_feat = tf.expand_dims(img_feat, 0)\n",
    "    predicted = '<start> '\n",
    "    # print(img_feat)\n",
    "    # print(curr_word)\n",
    "    for i in range(MAX_CAP_LEN):\n",
    "        # decoder will return array of vocabulary size(vocab_size) \n",
    "        predict = decoder(curr_word, img_feat)\n",
    "        # np.argmax(predict) will return index of max value, \n",
    "        # putting max value in input_vocab we get corresponding word\n",
    "        word_predict = input_vocab[np.argmax(predict)]\n",
    "        predicted += word_predict + ' '\n",
    "        # print(predicted)\n",
    "        curr_word = tf.expand_dims([np.argmax(predict)], 1)\n",
    "\n",
    "        # if 'predict' word contain <end> then we will return predicted till now\n",
    "        if word_predict == '<end>':\n",
    "            print('True Caption: ', img_cap)\n",
    "            return predicted\n",
    "    print('True Caption: ', img_cap)\n",
    "    return predicted\n",
    "idx = 41\n",
    "open_img(test[idx])\n",
    "print('Predicted Caption: ', evaluate(features[test[idx]], caption_mapping[test[idx]][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Egveae-IfvC7",
    "outputId": "4b3095a1-46aa-42a9-c4f4-68b8bc9c86ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'468930779_8008d90e10'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_AFquHKtMkK",
    "outputId": "f44efbbc-995e-4937-df77-315c73cdab77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f4d2934c190> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "decoder.save('/content/drive/MyDrive/Data/Image-Captioning-Data/model_30_sep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7XM0480Ewup"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
